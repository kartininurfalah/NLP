{"cells": [{"cell_type": "markdown", "source": ["We are using 2 datasets:\n", "1. Large Movie Review Dataset\n", "2. GloVe: Global Vectors for Word Representation"], "metadata": {"_cell_guid": "d7e3c3c7-e6ff-4010-ba78-e98636f8099d", "_uuid": "1d09548fa226d05e7a5fe2955e19266cdd195ab1"}}, {"cell_type": "markdown", "source": ["Sentiment Analysis = judging how positive or negative the tone in a document is. The output of a sentiment analysis is a score between zero and one, where one means the tone is very positive and zero means it is very negative. "], "metadata": {"_cell_guid": "15c4bf1d-dcaf-41cc-8d29-1b22ec418d77", "_uuid": "e1971b32b8b1abde00f23c20d6461992b11876fb"}}, {"outputs": [], "cell_type": "code", "source": ["from tqdm import tqdm"], "execution_count": null, "metadata": {"_cell_guid": "dfc95478-b518-4ae3-90b3-513322dd748a", "_uuid": "eafad3a2a69f0748548ed8ff623f764aa44c31f2", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["!ls ../input/glove-global-vectors-for-word-representation"], "execution_count": null, "metadata": {"_cell_guid": "ca4c053e-6145-440a-b3ef-077b85271be2", "_uuid": "811a56a33256190ce6893a53a3d3ecd25f492fe2"}}, {"outputs": [], "cell_type": "code", "source": ["import os\n", "imbd_dir = '../input/keras-imdb/aclImdb_v1/aclImdb'\n", "train_dir =os.path.join(imbd_dir, 'train')\n", "labels = []\n", "texts = []\n", "\n", "for label_type in ['neg', 'pos']:\n", "    dir_name = os.path.join(train_dir, label_type)\n", "    print('loading', label_type)\n", "    for fname in tqdm (os.listdir(dir_name)):\n", "        if fname[-4:] == '.txt':\n", "            f = open(os.path.join(dir_name, fname))\n", "            texts.append(f.read())\n", "            f.close()\n", "            if label_type == 'neg':\n", "                labels.append(0)\n", "            else:\n", "                labels.append(1)"], "execution_count": null, "metadata": {"_cell_guid": "4d8b150b-5858-4a77-b8d4-4671033fa97b", "_uuid": "44a05ef000931276914983dd7acd2ecd87405f22"}}, {"outputs": [], "cell_type": "code", "source": ["len(labels), len(texts)"], "execution_count": null, "metadata": {"_cell_guid": "c6e54e9e-c001-41e5-80b0-84a0693c8429", "_uuid": "3c07a84526b64c873ebbfbd5726cb74b58d2530f"}}, {"outputs": [], "cell_type": "code", "source": ["import numpy as np\n", "np.mean(labels)"], "execution_count": null, "metadata": {"_cell_guid": "5f718946-4772-4cff-92f5-563cd11de478", "_uuid": "c040fc82cab65a805ad3d1e7b7d91ef3cc01c862"}}, {"cell_type": "markdown", "source": ["**Tokens**"], "metadata": {"_cell_guid": "e756a1d0-1067-43d7-a935-99f58c5de5bb", "_uuid": "524dfe196270c66d1dbdb8537f8ea384259acb18", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["from keras.preprocessing.text import Tokenizer\n", "import numpy as np\n", "max_words = 10000\n", "tokenizer = Tokenizer(num_words = max_words)\n", "tokenizer.fit_on_texts(texts)\n", "sequences = tokenizer.texts_to_sequences(texts)"], "execution_count": null, "metadata": {"_cell_guid": "6be678c9-3047-4378-90ee-fb2f14caef3a", "_uuid": "9971df14f57488f5fa8e27c40bf2340e8886bf76"}}, {"outputs": [], "cell_type": "code", "source": ["word_index = tokenizer.word_index"], "execution_count": null, "metadata": {"_cell_guid": "5d9d5a1a-7ef2-46bc-bf09-fe51b1355170", "_uuid": "16d8654c9fba9c93532b6fb1615204427b416a24", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["from keras.preprocessing.sequence import pad_sequences\n", "maxlen = 100\n", "data = pad_sequences (sequences, maxlen = maxlen)\n", "print(data.shape)"], "execution_count": null, "metadata": {"_cell_guid": "f61937f5-d81c-481a-af45-9783a17fb1a1", "_uuid": "fc98e88e9b94a715eff462aa927f6a964cc45e6b"}}, {"outputs": [], "cell_type": "code", "source": ["labels = np.asarray(labels)\n", "indices = np.arange(data.shape[0])\n", "np.random.shuffle(indices)\n", "data = data[indices]\n", "labels = labels [indices]\n", "training_samples = 20000\n", "validation_samples = 5000\n", "\n", "x_train = data[:training_samples]\n", "y_train = labels[:training_samples]\n", "x_val = data[training_samples:training_samples+  validation_samples]\n", "y_val = labels[training_samples: training_samples + validation_samples]\n"], "execution_count": null, "metadata": {"_cell_guid": "339fa278-0b5d-4858-abcc-754d7df0b4c2", "_uuid": "23c4f0b7cb43922b20233d1355b00accdd47521a", "collapsed": true}}, {"cell_type": "markdown", "source": ["**Embeddings**"], "metadata": {"_cell_guid": "278e0011-bc2e-4dbf-8ade-0a118cb0ed5f", "_uuid": "db71c18d427b0452c10348746df083056ce352e6", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["from keras.models import Sequential\n", "from keras.layers import Embedding, Flatten, Dense\n", "embedding_dim = 50\n", "model = Sequential()\n", "model.add(Embedding(max_words, embedding_dim, input_length =maxlen))\n", "model.add(Flatten())\n", "model.add(Dense(32, activation='relu'))\n", "model.add(Dense(1, activation = 'sigmoid'))\n", "model.summary()"], "execution_count": null, "metadata": {"_cell_guid": "33b66b17-0200-4305-916b-bcd300da6bf8", "_uuid": "9a6ec6023002f724b190f7aeb22c5ba2a7b725d9"}}, {"outputs": [], "cell_type": "code", "source": ["model.compile(optimizer = 'adam',\n", "             loss='binary_crossentropy',\n", "             metrics=['acc'])"], "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["history = model.fit(x_train, y_train,\n", "                   epochs = 10,\n", "                   batch_size = 32,\n", "                   validation_data=(x_val, y_val))"], "execution_count": null, "metadata": {}}, {"outputs": [], "cell_type": "code", "source": ["glove_dir = '../input/glove-global-vectors-for-word-representation'\n", "print('Loading word vectors')\n", "embeddings_index = {}\n", "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n", "for line in tqdm(f):\n", "    values = line.split()\n", "    word = values[0]\n", "    embedding = np.asarray(values[1:], dtype='float32')\n", "    embeddings_index[word] = embedding\n", "f.close()\n", "print('Found %s word vectors'%len(embeddings_index))"], "execution_count": null, "metadata": {}}, {"outputs": [], "cell_type": "code", "source": ["all_embs = np.stack(embeddings_index.values())\n", "emb_mean = all_embs.mean()\n", "emb_std = all_embs.std()\n", "emb_mean, emb_std"], "execution_count": null, "metadata": {}}, {"outputs": [], "cell_type": "code", "source": ["embedding_dim = 100 # We now use larger embeddings\n", "\n", "word_index = tokenizer.word_index\n", "nb_words = min(max_words, len(word_index)) # How many words are there actually\n", "\n", "# Create a random matrix with the same mean and std as the embeddings\n", "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n", "\n", "# The vectors need to be in the same position as their index. \n", "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n", "\n", "# Loop over all words in the word index\n", "for word, i in word_index.items():\n", "    # If we are above the amount of words we want to use we do nothing\n", "    if i >= max_words: \n", "        continue\n", "    # Get the embedding vector for the word\n", "    embedding_vector = embeddings_index.get(word)\n", "    # If there is an embedding vector, put it in the embedding matrix\n", "    if embedding_vector is not None: \n", "        embedding_matrix[i] = embedding_vector"], "execution_count": null, "metadata": {}}, {"outputs": [], "cell_type": "code", "source": ["model = Sequential()\n", "model.add(Embedding(max_words, embedding_dim, input_length = maxlen, weights = [embedding_matrix], trainable = False))\n", "model.add(Flatten())\n", "model.add(Dense(32, activation = 'relu'))\n", "model.add(Dense(1, activation = 'sigmoid'))\n", "model.summary()"], "execution_count": null, "metadata": {}}, {"outputs": [], "cell_type": "code", "source": ["model.compile(optimizer = 'adam',\n", "             loss = 'binary_crossentropy',\n", "              metrics = ['acc'])"], "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["history = model.fit(x_train, y_train, \n", "                   epochs = 10,\n", "                   batch_size = 32,\n", "                   validation_data = (x_val, y_val))"], "execution_count": null, "metadata": {}}, {"outputs": [], "cell_type": "code", "source": ["my_text = 'I love dogs. Dogs are the best. They are lovely, cuddly animals that only want the best for humans.'\n", "seq = tokenizer.texts_to_sequences([my_text])\n", "print('raw seq:', seq)\n", "seq = pad_sequences(seq, maxlen=maxlen)\n", "print('padded seq:', seq)\n", "prediction = model.predict(seq)\n", "print('positivity:', prediction)"], "execution_count": null, "metadata": {}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["from os import listdir, makedirs\n", "from os.path import join, exists, expanduser\n", "\n", "cache_dir = expanduser(join('~', '.keras'))\n", "if not exists (cache_dir):\n", "                       makedirs(cache_dir)\n", "                       datasets_dir = join(cache_dir, 'datasets')\n", "                       \n", "if not exists(datasets_dir):\n", "                       makedirs(datasets_dir)\n", "\n", "!cp ../input/imdb* ~/.keras/datasets"], "execution_count": null, "metadata": {"_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19", "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["from keras.datasets import imdb\n", "from keras.preprocessing import sequence\n", "\n", "max_words = 10000 #how large our vocabulary is\n", "max_len = 500 #max_len -> cuts off text after 500 words\n", "\n", "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n", "print(len(x_train), 'train sequences')\n", "print(len(x_test), 'test sequences')\n"], "execution_count": null, "metadata": {"_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0", "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["word_index = imdb.get_word_index()"], "execution_count": null, "metadata": {"_cell_guid": "128fc13c-c220-412d-a168-cb885038afeb", "_uuid": "089141396fcfd85b546c24e9c9111489fd46cd55", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n", "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n", "print('x_train shape:', x_train.shape)\n", "print('x_test shape:', x_test.shape)"], "execution_count": null, "metadata": {"_cell_guid": "b3ff912d-2837-4c69-a8e4-bebc9de7f912", "_uuid": "029dad52f7c287f5072b65e8b7d350151f9bb971", "collapsed": true}}, {"cell_type": "markdown", "source": ["Starting with building the convolutional model\n", "\n", "MaxPooling1D -> takes a piece of sequence with specified length and returns the maximum element in the sequence. MaxPooling always returns the maximum element for each channel. \n", "GlobalMaxPooling2D -> returns the maximum over the entire sequence. \n", "MaxPooling1D significantly shortens the sequence and GlobalMaxPooling2D removes the temporal dimension entirely."], "metadata": {"_cell_guid": "b87a5b42-7ec3-4393-bba0-d4a89f55d190", "_uuid": "7cc748fc8026236c89f10437dd7f279614adac58"}}, {"outputs": [], "cell_type": "code", "source": ["embedding_dim = 100"], "execution_count": null, "metadata": {"_cell_guid": "4b576935-cc16-4823-9858-ac61e99b08f2", "_uuid": "7c33aafef9a0f5e8fcbc789421af70bb42871a81", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["from keras.models import Sequential\n", "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n", "\n", "model = Sequential()\n", "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n", "model.add(Conv1D(32, 7, activation='relu'))#32 channgels, window size 7\n", "model.add(MaxPooling1D(5)) #pool windows of size 5\n", "model.add(Conv1D(32, 7, activation = 'relu'))\n", "model.add(GlobalMaxPooling1D()) \n", "model.add(Dense(1)) #final output layer\n", "\n", "model.summary()"], "execution_count": null, "metadata": {"_cell_guid": "3a0fd89b-0d3b-405d-8815-1a283666f00e", "_uuid": "51b5e36e5b790325ca8d6dc1c88a6b8c29670188", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["model.compile(optimizer = 'adam',\n", "             loss='binary_crossentropy',\n", "             metrics = ['acc'])\n", "\n"], "execution_count": null, "metadata": {"_cell_guid": "c7b92b1b-98ba-4ffe-a34d-938e65747171", "_uuid": "3b9f90eb6eac7b60c04a72ec037d4068d35bb93a", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["history = model.fit (x_train, y_train,\n", "                    epochs = 2,\n", "                    batch_size = 128,\n", "                    validation_split=0.2)"], "execution_count": null, "metadata": {"_cell_guid": "f0d83bf9-27e4-4136-818e-d6b5711fb4d5", "_uuid": "d749dbde9b25504fce8a0319a48b0dcb88d7e359", "collapsed": true}}, {"cell_type": "markdown", "source": ["**Reoccurent Neural Networks**: can remember their last activation and use it as their own output. A reocurrent layer takes a sequence as an input. For each element, it then computes a matrix multiplication just like a Dense layer and runs the result through an activation function like relu. It then retains its own activation. When the next item of the sequence arrives, it performs the matrix multiplication as before but it also multiplies it's previous activation wieh a second matrix. It adds the result of both operations together and passes it through its activation function again."], "metadata": {"_cell_guid": "8fe16f3a-af30-4bed-83e6-44971217dc40", "_uuid": "3d20fbbef8a9e406f94425863ec3bf7325bf1483"}}, {"outputs": [], "cell_type": "code", "source": ["from keras.layers import SimpleRNN"], "execution_count": null, "metadata": {"_cell_guid": "2fe3c874-db3b-48dd-b2ac-2b2863ba3aca", "_uuid": "40295b8060311481a35cd61915a23485169c3962", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["model = Sequential ()\n", "model.add(Embedding(max_words, embedding_dim))\n", "model.add(SimpleRNN(32, activation= 'relu'))\n", "#model.add(SimpleRNN(23, activation= 'relu', return_sequences = True))\n", "model.add(Dense(1))\n", "\n", "model.summary()"], "execution_count": null, "metadata": {"_cell_guid": "da38a2e1-cbc1-4327-8481-369c226976f9", "_uuid": "a290ba74864c668d324b00912c38ba0ff9f36f43", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["model.compile(optimizer='adam',\n", "             loss='binary_crossentropy',\n", "             metrics=['acc'])"], "execution_count": null, "metadata": {"_cell_guid": "4727be0a-1e07-461f-b476-a845aa785629", "_uuid": "9f75351de5d2f9151a3b2dda84cf0eb7f72e0f90", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": ["history = model.fit(x_train, y_train,\n", "                   epochs = 5,\n", "                   batch_size = 128,\n", "                   validation_split = 0.2)"], "execution_count": null, "metadata": {"_cell_guid": "f401b24c-3199-43f4-b80e-101fd1dca71d", "_uuid": "2ec29fa965494edb2acb391dbd87b7e5693bb40c", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"_cell_guid": "ed21f09f-68f4-4d91-ad5b-873f46332b00", "_uuid": "1d8f781396a4765824f95e143ccdd2b16247a1cf", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"_cell_guid": "399b97dd-3824-43e9-b96a-5a605147dc76", "_uuid": "e1d8c09b6d222551a8b78df300ed1997320e255c", "collapsed": true}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"_cell_guid": "5aa9b127-9fa6-4347-9779-27e6269f0732", "_uuid": "6bf9d34934c5f4f246855c1cf46c78b54fb800ec", "collapsed": true}}, {"cell_type": "markdown", "source": [], "metadata": {"_cell_guid": "a933ca4b-b35b-451e-ab24-79301c3c6eb6", "_uuid": "03318b3f0c561eacb88d1352e73f6c922d884d15"}}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"version": "3.6.4", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python"}}}